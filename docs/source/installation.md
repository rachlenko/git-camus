# Installation

This guide will help you install and configure Git-Camus to work with your local Ollama service.

## Prerequisites

- Python 3.9 or higher
- [Ollama](https://ollama.ai/) installed and running locally
- A compatible language model (e.g., llama3.2, codellama, etc.)

## Step 1: Install Ollama

### macOS and Linux

```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### Windows

1. Download the installer from [https://ollama.ai/download](https://ollama.ai/download)
2. Run the installer and follow the setup wizard
3. Add Ollama to your system PATH if prompted

### Verify Installation

```bash
ollama --version
```

## Step 2: Start Ollama and Pull a Model

### Start the Ollama Service

```bash
ollama serve
```

This starts the Ollama API server on `http://localhost:11434` by default.

### Pull a Language Model

In a new terminal, pull a model:

```bash
# Recommended: llama3.2 (good balance of speed and quality)
ollama pull llama3.2

# Alternative: codellama (optimized for code)
ollama pull codellama

# Alternative: mistral (fast and efficient)
ollama pull mistral
```

## Step 3: Install Git-Camus

### From PyPI (Recommended)

```bash
pip install git-camus
```

### From Source

```bash
git clone https://github.com/rachlenko/git-camus.git
cd git-camus
pip install -e .
```

## Step 4: Configure Git-Camus (Optional)

Git-Camus uses sensible defaults, but you can customize the behavior:

### Environment Variables

```bash
# For Linux/macOS
export OLLAMA_HOST="http://localhost:11434"  # Ollama service URL
export OLLAMA_MODEL="llama3.2"               # Model to use

# For Windows (Command Prompt)
set OLLAMA_HOST=http://localhost:11434
set OLLAMA_MODEL=llama3.2

# For Windows (PowerShell)
$env:OLLAMA_HOST="http://localhost:11434"
$env:OLLAMA_MODEL="llama3.2"
```

### Configuration File

Create a `.git-camus.toml` file in your project root or home directory:

```toml
[ollama]
host = "http://localhost:11434"
model = "llama3.2"

[generation]
temperature = 0.7
max_tokens = 150
```

## Step 5: Test the Installation

1. Create a test repository or navigate to an existing one
2. Make some changes and stage them:
   ```bash
   echo "test" > test.txt
   git add test.txt
   ```
3. Run git-camus:
   ```bash
   git-camus --show
   ```

You should see a philosophical commit message generated by your local Ollama service.

## Troubleshooting

### Common Issues

1. **"Could not connect to Ollama"**
   - Make sure Ollama is running: `ollama serve`
   - Check if the service is accessible: `curl http://localhost:11434/api/tags`

2. **"Model not found"**
   - Pull the model: `ollama pull llama3.2`
   - List available models: `ollama list`

3. **Slow responses**
   - Try a smaller model
   - Check your system resources
   - Consider using a more powerful machine

4. **Permission errors**
   - Make sure you have write permissions in the repository
   - Check if Git is properly configured

### Model Recommendations

- **llama3.2**: Good balance of speed and quality for most use cases
- **codellama**: Optimized for code-related tasks and commit messages
- **mistral**: Fast and efficient for shorter responses
- **llama3.1**: Smaller and faster, good for quick commits

### Performance Tips

- Use an SSD for faster model loading
- Ensure adequate RAM (8GB+ recommended)
- Consider using a GPU if available for faster inference
- Close other resource-intensive applications when running Ollama

## Next Steps

Once installation is complete, you can:

1. Read the [Usage Guide](usage.md) to learn how to use Git-Camus
2. Explore the [API Documentation](api.md) for advanced configuration
3. Check out the [Philosophy Guide](philosophy.md) to understand the existentialist approach
